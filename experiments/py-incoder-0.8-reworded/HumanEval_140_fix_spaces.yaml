name: HumanEval_140_fix_spaces
language: py
prompt: "def fix_spaces(text: str) -> str:\n    \"\"\"\n    Given a string text, replace\
  \ all spaces in it with underscores, \n    and if a string has more than 2 consecutive\
  \ spaces, \n    then replace all consecutive spaces with - \n    \n    >>> fix_spaces('\
  \ Example')\n    'Example'\n    >>> fix_spaces(' Example 1')\n    'Example_1'\n\
  \    >>> fix_spaces(' Example 2')\n    '_Example_2'\n    >>> fix_spaces(' Example\
  \ 3')\n    '_Example-3'\n    \"\"\"\n"
tests: "def check(candidate):\n    assert candidate('Example') == 'Example'\n    assert\
  \ candidate('Mudasir Hanif ') == 'Mudasir_Hanif_'\n    assert candidate('Yellow\
  \ Yellow  Dirty  Fellow') == 'Yellow_Yellow__Dirty__Fellow'\n    assert candidate('Exa\
  \   mple') == 'Exa-mple'\n    assert candidate('   Exa 1 2 2 mple') == '-Exa_1_2_2_mple'\n\
  \ndef test_check():\n    check(fix_spaces)\n\ntest_check()\n"
completions: ["    # Add an underscore before any sequence of spaces\n    \n    return\
    \ re.sub(' +', '_', text)\n\n", "    \n    text = re.sub(' +',\"_\",text)\n  \
    \  text = re.sub('\\s{2,}', '_', text)\n    \n    return text\n</cell>\n<cell>\n\
    train_df['id'] = train_df['id'].apply(fix_ids)\ntrain_df['text_lemmatized'] =\
    \ train_df['text_lemmatized'].apply(fix_spaces)\ntrain_df['title_lemmatized']\
    \ = train_df['title_lemmatized'].apply(fix_spaces)\ntrain_df.head(4)\n</cell>\n\
    <text>\nSave the cleaned data\n\n\n</text>\n<cell>\ntrain_df.to_csv(path_or_buf='data/amazon_reviews_us_Digital_Software_v1_00_train.csv',\
    \ header=True, index=False, quoting=csv.QUOTE_NONNUMERIC)\n</cell>\n<text>\nPreparing\
    \ the Test Data\n\n</text>\n<cell>\ntest_df = pd.read_csv('./data/amazon_reviews_us_Digital_Software_v1_00_test_public.csv',\
    \ \n                      dtype=str, \n                      error_bad_lines=False,\
    \ \n                      warn_bad_lines=True)\n\ntest_df['id'] = test_df['id'].apply(fix_ids)\n\
    test_df['text_lemmatized'] = test_df['text_lemmatized'].apply(fix_spaces)\ntest_df['title_lemmatized']\
    \ = test_df['title_lemmatized'].apply(fix_spaces)\ntest_df.head()\n</cell>\n<cell>\n\
    test_df.to_csv(path_or_buf='data/amazon_reviews_us_Digital_Software_v1_00_test.csv',\
    \ header=True, index=False, quoting=csv.QUOTE_NONNUMERIC)\n</cell>\n<cell>\n!pip\
    \ install konlpy wordcloud\n</cell>\n<text>\nLoad in the cleaned Test data\n\n\
    \n</text>\n<cell>\ntest_df = pd.read_csv('./data/amazon_reviews_us_Digital_Software_v1_00_test.csv',\
    \ \n                      dtype=str, \n                      error_bad_lines=False,\
    \ \n                      warn_bad_lines=True)\n\ntest_df['id'] = test_df['id'].apply(fix_ids)\n\
    test_df['text_lemmatized'] = test_df['text_lemmatized'].apply(fix_spaces)\ntest_df['title_lemmatized']\
    \ = test_df['title_lemmatized'].apply(fix_spaces)\ntest_df.head()\n</cell>\n<cell>\n\
    from konlpy.tag import Okt\n\nokt = Okt()\n", "    return re.sub('\\s+', '_',\
    \ text)\n\n\n@lru_cache(maxsize=1024)", "    \n    tokens = text.split()\n   \
    \ \n    # replace spaces with underscores\n    for i in range(len(tokens)):\n\
    \        if tokens[i].isspace():\n            tokens[i] = tokens[i].replace('\
    \ ', '_')\n    \n    # replace double consecutive spaces with -\n    space_counter\
    \ = 0\n    \n    for i in range(len(tokens)):\n        if tokens[i].isspace():\n\
    \            if space_counter > 0:\n                tokens[i] = '-' + tokens[i]\n\
    \                space_counter = 0\n        else:\n            space_counter +=\
    \ 1\n    \n    return ' '.join(tokens)\n\n", "    return ' '.join(text.split())\n",
  "    if text.find(' ') >= 0:\n        spaces = re.sub(' +', ' ', text)\n       \
    \ consecutive = re.sub(' +', ' ', spaces)\n        no_space = re.sub(' +', '',\
    \ consecutive)\n        no_space = re.sub('-+', '-', no_space)\n        no_space\
    \ = re.sub(' +', '', no_space)\n        return no_space\n    else:\n        return\
    \ text\n\n", "    return re.sub(' +', ' ', text)\n\n", "    \n    spaces = [i\
    \ for i in text if i in ' ']\n    if len(spaces) >= 2:\n        return '_'.join(spaces)\n\
    \    else:\n        return text\n</cell>\n<text>\nTest your work\nWith this function,\
    \ you can (mostly) automate the process of fixing text data.\n\n\n</text>\n<cell>\n\
    fix_spaces(' Example')\n</cell>\n<cell>\nfix_spaces(' Example 1')\n</cell>\n<cell>\n\
    fix_spaces(' Example 2')\n</cell>\n<cell>\nfix_spaces(' Example 3')\n</cell>\n\
    <text>\nOptional: Removing punctuation\nSome text analysis tasks consider punctuation\
    \ to be an important aspect of a word. \nIf you're working with some text that\
    \ includes punctuation, like names, addresses, \nor phone numbers, you might want\
    \ to remove it.\n\nFor example, text that might be a\n\nname or email\naddress,\n\
    phone number,\nsocial security number\n\nmight contain punctuation like commas\
    \ and periods. \nOther tasks might want to remove all punctuation. \nIt's okay\
    \ to remove punctuation if and only if it doesn't hurt the semantics of the text.\
    \ \nFor this optional task, you are welcome to specify a list containing the specific\
    \ punctuation you want to remove. \n(Later on in this specialization, you'll learn\
    \ how to include other punctuation in your \nlist and how to handle text that\
    \ includes multiple consecutive punctuations. You'll see how to do that later.)\n\
    \nInstructions\n\nIn the list below, write a list of the punctuations that you\
    \ want to remove. \nNote that punctuations are like commas and periods, so be\
    \ sure to include them in the list.\n\n\n</text>\n<cell>", "    return re.sub('\\\
    s+', '_', text)\n</cell>\n<cell>\nprint(fix_spaces(' Example'))\nprint(fix_spaces('\
    \ Example 1'))\nprint(fix_spaces(' Example 2'))\nprint(fix_spaces(' Example 3'))\n\
    </cell>\n<cell>", "    text = re.sub(r'\\s+', '_', text).strip()\n    return re.sub(r'_{2,}',\
    \ '_', text)\n", "    return text.replace(' ', '-')\n\n", "    text = text.replace('\
    \ ', '_')\n    if len(text.split(' ')) > 1:\n        space_replacement = ''\n\
    \        for space in text.split(' '):\n            if space != text.replace(space,\
    \ '_'):\n                space_replacement += space + '_'\n        text = text.replace(space_replacement,\
    \ '')\n    return text\n\n", "    return text.replace(' ', '_').replace('  ',\
    \ '-')\n", "    # remove double spaces\n    text = re.sub(r'  +', ' ', text)\n\
    \    # replace consecutive spaces with a single -\n    text = re.sub(r'  +', '-',\
    \ text)\n    return text\n", "    return re.sub(' +', ' ', text)\n", "    pattern\
    \ = re.compile(r\"\\s(?=\\s|$)\", re.UNICODE)\n    return pattern.sub(lambda m:\
    \ \"_\", text)\n</cell>\n<text>\nLet's test this function on some examples:\n\n\
    \n</text>\n<cell>\nfor i in range(100):\n    print(fix_spaces(\"Example {}\".format(i)),\
    \ end=', ')\n</cell>\n<text>\nLet's also define some custom classes to play around\
    \ with:\n\n\n</text>\n<cell>", "    if text.find(' ') != -1:\n        spaces =\
    \ text.find(' ')\n        spaces_left = text.find(' ', spaces + 1)\n        spaces_right\
    \ = text.find(' ', spaces + 2)\n        text = text.replace(text[spaces:spaces_left],\
    \ '_')\n        text = text.replace(text[spaces_left:spaces_right], '-')\n   \
    \ return text\n\n", "    \n    if text == '':\n        return ''\n    \n    spaces\
    \ = text.count(' ')\n    \n    if spaces > 2:\n        new_spaces = '-' * spaces\n\
    \        text = ''.join([s.strip() for s in text.split(' ')]) + new_spaces\n \
    \   \n    return text\n\n", "    words = text.split()\n    for i in range(len(words)):\n\
    \        if words[i].endswith(' '):\n            words[i] = words[i][0:-1]\n \
    \   if len(words) == 1:\n        return text\n    elif len(words) == 2 and words[1]\
    \ == '':\n        return text\n    else:\n        return ' '.join(words[1:])\n\
    \n", "    \n    pattern = re.compile(r' +')\n    return pattern.sub('_', text)\n\
    \n"]
stop_tokens:
- "\ndef"
- "\n#"
- "\nif"
- "\nclass"
