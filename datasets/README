

- /originals: The HumanEval dataset from Chen et al (2021) with three benchmarks
  excluded and type annotations added to all benchmarks that did not have them.
  See our paper for details.

- /originals_with_cleaned_doctests: A version of `/originals` where all ad hoc
  examples in docstrings rewritten in Python's doctest format.

- /originals_with_cleaned_vocab_L: A version of `/originals_with_cleaned_doctests`
  where Python-specific terminology is replaced with L-specific terminology in the
  doctests. This was automated with the `../translate_terminology` scripts.

- /mbpp: The MBPP dataset from Austin, Odena, et al (2021). We have reformatted
  it to use the same format as the HumanEval dataset. It is presently unused.

## References

Chen et al. Evaluating Large Language Models Trained on Code. 2021.
https://arxiv.org/abs/2107.03374

Austin, Odena, et al. Program Synthesis with Large Language Models. 2021.
https://arxiv.org/abs/2108.07732
